# -*- coding: utf-8 -*-
"""ptt資料爬蟲.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14GkFa91x4N75CufBXPE_0xuEbLvo0J-S

# PTT Gossiping Crawling

---

**PTT Gossiping Scraping **

Get the insights you need from the PTT Gossiping community at your fingertips. It provides a comprehensive and automated way to extract valuable data from one of the most popular online forums in Taiwan - PTT's Gossiping board.

My application is designed to search for specific topics, in this case, "疫苗副作用" (which translates to "Vaccine Side Effects"). It then scrapes the relevant data from the post content and the associated comments, including the post's ID, platform, title, date, and the comments' ID, text, and date.

Features:

1. **Flexible Data Collection**: Easily adjust the number of pages to scrape, making it adaptable for both small scale and large scale data extraction.

2. **Robust Error Handling**: Carefully designed to handle missing or unexpected data, ensuring a smooth and uninterrupted scraping process.

3. **Data Transformation**: Transforms raw scraped data into structured pandas DataFrames, making the data ready for analysis, visualization, or storage.

4. **Scalability**: Can be extended to scrape other boards on PTT or adapted to other websites as well.
"""

from tqdm import tqdm
import requests
from bs4 import BeautifulSoup
import re
import datetime
import pandas as pd

def create_session(url, payload):
    """
    Create and return a requests session.
    """
    session = requests.session()
    session.post(url, data=payload)
    return session

def get_page(session, url):
    """
    Request a page using the session and return its content parsed with BeautifulSoup.
    """
    response = session.get(url)
    return BeautifulSoup(response.text, "html.parser")

def parse_post(post_link, year):
    """
    Extract post data from a post link.
    """
    if post_link.a and "href" in post_link.a.attrs:
        page_url = "https://www.ptt.cc/" + post_link.a["href"]
        result = get_page(session, page_url)

        main_content = result.find("div", id="main-content")
        article_info = main_content.find_all("span", class_="article-meta-value")

        if len(article_info) != 0:
            post_id = post_link.a["href"].split('/')[-1].split('.')[1]
            platform = 'PTT'
            post_title = article_info[2].string
            post_date = datetime.datetime.strptime(article_info[3].string, '%a %b %d %H:%M:%S %Y').date()

            post = {'Post_ID': post_id, 'Platform': platform, 'Post_Title': post_title, 'Post_Date': post_date}

            comments = parse_comments(main_content, post_id, post_date.year)

            return post, comments

def scrape_pages(start_url, num_pages):
    """
    Scrape a number of pages from a start url and return the extracted posts and comments data.
    """
    url = start_url
    posts_data = []
    comments_data = []

    for _ in range(num_pages):
        soup = get_page(session, url)

        links = soup.find_all("div", class_="title")
        for link in links:
            if link.a and "href" in link.a.attrs:
                post_comments = parse_post(link, year)
                if post_comments is not None:
                    post, comments = post_comments
                    posts_data.append(post)
                    comments_data.extend(comments)

        prev_page_link = soup.find("a", string="‹ 上頁")
        if prev_page_link and "href" in prev_page_link.attrs:
            url = "https://www.ptt.cc/" + prev_page_link["href"]
        else:
            break

    return posts_data, comments_data


def parse_comments(main_content, post_id, year):
    """
    Extract comments data from the post content.
    """
    comments_data = []
    comments = main_content.find_all('div', class_='push')
    for idx, comment in enumerate(comments):
        comment_date = comment.find('span', class_='push-ipdatetime').text.strip()
        comment_date = re.sub(r'\d+\.\d+\.\d+\.\d+ ', '', comment_date)  # Remove IP address
        comment_date = datetime.datetime.strptime(comment_date, '%m/%d %H:%M')
        comment_date = comment_date.replace(year=year).date()
        comment_text = comment.find('span', class_='push-content').text.strip()[1:]
        comments_data.append({'Comment_ID': f'{post_id}_{idx}', 'Post_ID': post_id, 'Comment_Text': comment_text, 'Comment_Date': comment_date})

    return comments_data


url = "https://www.ptt.cc/bbs/Gossiping/search?q=疫苗副作用"  #Modify after "q=" to scrap the result you want
payload = {'from': '/bbs/Gossiping/index.html', 'yes': 'yes'} #PTT八卦版
session = create_session("https://www.ptt.cc/ask/over18", payload) #To post the payload to pass the age 18 limitation
year = datetime.datetime.now().year

posts_data, comments_data = scrape_pages(url, 5)

posts_df = pd.DataFrame(posts_data)
comments_df = pd.DataFrame(comments_data)

print(posts_df)
print(comments_df)

"""# Important libraries and function of each variables:

1. **Import necessary libraries**: The required Python libraries are imported at the beginning of the script. These are:

   - `tqdm`: Used to show a progress bar when looping over a large number of items.
   - `requests`: Used to make HTTP requests to the website.
   - `BeautifulSoup`: Used to parse HTML content.
   - `re`: The standard Python module for regular expressions, used to perform string manipulations.
   - `datetime`: Used to handle date and time related tasks.
   - `pandas`: A powerful data manipulation library.

2. **Define functions**:

   - `create_session(url, payload)`: This function creates a new session, posts the payload to the URL, and returns the session.
   - `get_page(session, url)`: This function sends a GET request to the specified URL using the session created earlier, and parses the HTML response using BeautifulSoup.
   - `parse_post(post_link, year)`: This function takes a link to a post and the current year as inputs, extracts post data, and also calls `parse_comments` function to extract comments data from the post.
   - `scrape_pages(start_url, num_pages)`: This function scrapes a specified number of pages from the starting URL. It uses `get_page` to fetch the page content and `parse_post` to extract the post and comments data.
   - `parse_comments(main_content, post_id, year)`: This function extracts comments data from a post's content.

3. **URL and payload**: The URL for the search query and the payload for the post request are defined.

4. **Create session**: A session is created using the `create_session` function and the URL and payload defined earlier. This session is used for all subsequent requests to maintain the same HTTP context.

5. **Get the current year**: The current year is fetched using the `datetime` library. This is used later when parsing the dates of the comments.

6. **Scrape pages**: The `scrape_pages` function is called with the search URL and the number of pages to scrape. It returns two lists containing the posts data and the comments data respectively.

7. **Create DataFrames**: The lists of posts and comments data are converted into pandas DataFrames for easier manipulation and analysis.

8. **Print DataFrames**: Finally, the DataFrames are printed to the console.

Some important variables in the code are:

- `session`: This is an instance of a `requests.Session` that is used to send requests to the website. A session is used instead of individual requests to keep the context (like cookies) across multiple requests.
- `year`: The current year, used when parsing the dates of the comments.
- `posts_data` and `comments_data`: These lists store the data of posts and comments scraped from the website.
- `posts_df` and `comments_df`: These are pandas DataFrames that store the posts and comments data in a tabular format.

# Saving the result
"""

posts_df = pd.DataFrame(posts_data)
posts_df.to_csv('posts_df.csv', index=False)

comments_df = pd.DataFrame(comments_data)
comments_df.to_csv('comments_df.csv', index=False)